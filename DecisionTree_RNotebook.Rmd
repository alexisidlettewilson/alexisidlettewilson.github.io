---
title: "Decision Tree Methods"
output:
  html_notebook:
    fig_height: 6
  html_document: default
  pdf_document: default
  word_document: default
---

# Decision Tree Method for Classification
[Back to Homepage](https://alexisidlettewilson.github.io/)

I use the ISLR library to get the `Carseats` data set. I also install the `tree` package. 

library(ISLR)
data("Carseats")
install.packages("tree")
library(tree)

## Decision Tree for Classification

I create a histogram of sales to help decide what the classification targets will be. The values on the x-axis correspond to thousands of dollars.
```{r}
attach(Carseats)
hist(Sales)
```

Based on the histogram, I create a binary variable to split the Sales into "high" and "low" at the $8,000 mark. 
```{r}
salesVol <- ifelse(Sales<=8,"No","Yes")
```

I  create a new data frame to hold the `Carseats` data and the labels for each observation.
```{r}
Carseats <- data.frame(Carseats,salesVol)
head(Carseats)
```

I create a decision tree model using the new `Carseats` data frame. I make sure to exclude "Sales" from the model since my salesVol variable is directly based on "Sales".

I also get a summary of my decision tree model.
```{r}
tree.carseats <- tree(salesVol~.-Sales, data=Carseats)
summary(tree.carseats)
```

I end up with 27 terminal nodes. I can actually plot the model to see a tree diagram. 

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0,  cex = 0.45)
```
The value under each terminal indicates the predicted value for observations in that segment. The height of each node indicates how much the misclassification error rate was reduced by the split. The improvement in misclassification error rate gets smaller as we make more splits.

## Test and Training

Create a training data set with 250 rows. There are 400 observations total.
```{r}
set.seed(1011)
train = sample(1:nrow(Carseats),250)
```

Create and plot a decision tree using the training data.
```{r}
tree.carseatsTrainModel <- tree(salesVol~.-Sales, data = Carseats, subset=train)
plot(tree.carseatsTrainModel)
text(tree.carseatsTrainModel, pretty=0, cex=0.45)
```

Predict values for the remaining test data. We specify the type as classification to predict the class labels.
```{r}
tree.pred <- predict(tree.carseatsTrainModel, Carseats[-train,], type="class")
```

Evaluate the error
```{r}
with(Carseats[-train,], table(tree.pred,salesVol))
```
```{r}
(72+33)/150
```
The misclassification error rate is 0.30.

Use cross validation *cv* to prune the tree. We indicate we will use misclassification error in the parameters.
```{r}
cv.carseats <- cv.tree(tree.carseatsTrainModel, FUN=prune.misclass)
plot(cv.carseats)

```

I pick 13 as the number of terminal nodes as it is in the middle of the area with lowest deviance.
```{r}
prune.carseats <- prune.misclass(tree.carseatsTrainModel, best=13)
plot(prune.carseats)
text(prune.carseats, pretty=0, cex=0.7)
```

I will use this new "pruned" model on the test data set, then evaluate the error.
```{r}
tree.pred.Pruned <- predict(prune.carseats, Carseats[-train,], type="class")
with(Carseats[-train,], table(tree.pred.Pruned,salesVol))
```
```{r}
(72+32)/150
```

I find that the error rate did not improve with the pruned model.